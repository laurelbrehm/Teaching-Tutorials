---
title: "Contrasts"
author: "Laurel Brehm"
output: html_document
---
  
```{r libraries, include=FALSE}
## load previously installed packages into library 
library(lme4) #for mixed effect models
library(tidyverse)  #plotting, tabulating, etc.
library(languageR)  #the data set's in here
library(markdown)

## install plyr-- but DO NOT LOAD
## install.packages('plyr')
```

For the first set of analyses, we'll use simulated data. I did this because (1) simulating data gives you full control over the effects you expect, so it is a nice teaching tool. And because (2) it's nice to continue using the same data set through examples

This code is in the file 'pmer_simulatedLexDec.R'- you can look at it on your own time.
The data is: simulatedLexDec.txt -- download and save to the folder this code lives in.

We will again be working with Frequency and Native Language-- but I made up some data for how native language might affect lexical decision time.

First we will read in the data and make a box plot of means by the new simulated native language factor to see what it looks like.

```{r setup, include=T}
lexdecSm <- read.table('simulatedLexDec.txt',header=T)

### make a boxplot of means
## (note that Dutch is the leftmost-- alphabetical assignment)
ggplot(lexdecSm,aes(x=NativeLangSim,y=RT,fill=NativeLangSim))+
  geom_boxplot()
```


Contrasts are what allows you to interpret categorical x variables in a regression (think: lines) context.  It's easy to draw a line associating some y for some x that is 1, 2, or 3.  But how do you draw a line associating some y for some x that is 'English', 'German', and 'Spanish' ?  We assign them numbers.

In particular, we will assign the values numbers that have some relation to x=0.  That is because the intercept reflects the estimated effect when x=0.

We'll start by looking at the continuous predictor Frequency. While continuous predictors are already numeric, for reasons we'll see soon, it is nice to have zero set to the middle of the range.  This is called 'centering'. Do this by subtracting the mean from all values.

```{r centering, include=T}
lexdecSm$FreqC <- lexdecSm$Frequency - mean(lexdecSm$Frequency)
```

Next, you might be thinking-- we ran analyses with NativeLanguage in it already!  Yes. This is because R sets default contrasts.  Look at them with this code. Note that R's default is dummy coding with the alphabetically-first level as reference level.

Examining this matrix, we see that we have k-1 comparisions, where k is the number of levels of the variable. Each comparison is weighted using the contrast matrix: 1 means the comparison 100% reflects the effect at this level. It is often nice for interpretation to have the absolute value of the contrast weights to sum to 1 (though not essential, as we'll see later).

```{r defaults, include=T}
contrasts(lexdecSm$NativeLangSim)
```

## Sum contrasts 

This contrast matrix is of little use for us!  Instead, let's try a sum coding contrast scheme. Compare Dutch to English, German to Spanish, and the average of English and Dutch to the average of German and Spanish.  This will allow us to test whether the most-similar language is different from English, and whether the two most-different languages are different from each other. Set these, and run a new analysis.

To run this, we will make it so that zero lives in the middle of each of our comparisons, by making them positive and negative 1/2.

The contrasts here still follow the guideline we set before: absolute values sums to 1 (100%, not more, not less)-- but the 1 here is divided across two levels. And here, the sum of the whole contrast is zero.

We are running a model with a random slope for Frequency only by Subject. There is not so much variance to be captured with a random slope for NativeLangSim by Word, making the estimates small and highly-correlated, and the model getting some convergence failures (try it yourself to see, if you like!)

```{r sum, include=T}
## make new contrast matrix with named values
## my own personal convention = label has positive value first
EngvDutch <- c(-1/2,1/2,0,0)  ## Dutch = -1/2, Eng = 1/2
GervSpa <- c(0,0,1/2,-1/2) ## Ger = 1/2, Spa = -1/2
EngDuvGerSpa <- c(1/2,1/2,-1/2,-1/2) ## Dutch = 1/2, Eng= 1/2, Ger=-1/2 Spa=-1/2

SumZero <- cbind(EngvDutch,GervSpa,EngDuvGerSpa)
rownames(SumZero)=c("Dutch","English","German","Spanish")

## set these contrasts
contrasts(lexdecSm$NativeLangSim) <- SumZero

### run analysis
lmSumZero <- lmer(RT~NativeLangSim*FreqC + (1+FreqC|Subject)+(1 |Word),data=lexdecSm)
summary(lmSumZero)

### plot to unpack interaction!
## use revalue function from plyr package to make new var that sets up contrast we want in display
lexdecSm$NLcontr3 <- plyr::revalue(lexdecSm$NativeLangSim,c("English"="EngDu","Dutch"="EngDu","German"="GerSpa","Spanish"="GerSpa"))

ggplot(lexdecSm,aes(x=FreqC,y=RT,color=NativeLangSim))+
  geom_point(alpha=.1) + geom_smooth(color='black',method='lm') +
  geom_smooth(method='lm') + facet_grid(~NLcontr3)

```

We observed that the overall intercept estimate was 6.89.  The main effect of EngvDutch is -.20-- the intercept for this condition (black line) is in 6.69.  The main effect of GervSpa is .19-- the intercept for this condition (black line) is 7.08.  You can see this in the plot. 

The main effect of Frequency is -0.05.  That means: on average, across all language groups, the effect of frequency is decreasing by .06 for each unit of y (RT in log milliseconds)

We also observed a large EngDu versus GerSpa interaction. Note that the black lines in both panels have different intercepts. That's all that means! The effect estimate is negative here-- this means that the level assigned to be a positive number in the contrasts (EngDu) is smaller than the one assigned to be a positve number (GerSpa). In fact, the intercept estimate is .54 smaller (the beta).

And, there is an EngDu by Frequency interaction. This means that the red line has a steeper slope than the green line in the left panel... steeper by beta (0.03) much. 


## Treatment/Dummy contrasts
Maybe the question you care about is: are lexical decisions different for people whose native language is not English, with English being the baseline or reference comparsion. To test that, we use dummy coding (/baseline coding). For this, pick one level that always gets a 0, (here: English, number 2 in the matrix), and then the n-1 contrasts each have a 1 in the level to be compared to the intercept.

Note that the code for the model itself is the same as above. All that we are changing is the contrasts going in-- and this in turn, affects what the model means.

```{r dummy, include=T}
#### Set treatment/dummy coding contrasts by hand####
### now re-run with reference level = English
## make new contrast matrix with named values
Dutch <- c(1,0,0,0) 
Ger <- c(0,0,1,0)
Spa <- c(0,0,0,1)

Ref <- cbind(Dutch,Ger,Spa)
rownames(Ref)=c("Dutch","English","German","Spanish")

## set these contrasts
contrasts(lexdecSm$NativeLangSim) <- Ref

### run analysis
lmRef <- lmer(RT~NativeLangSim*FreqC + (1+FreqC|Subject)+(1|Word),data=lexdecSm)
summary(lmRef)

## unpack interaction: as grid plot
ggplot(lexdecSm,aes(x=Frequency,y=RT,color=NativeLangSim))+
  geom_point(alpha=.1) + geom_smooth(method='lm') + facet_grid(~NativeLangSim)

```

In this model, the intercept is 6.32.  This is the intercept for the English condition-- not the average!  Note that the effect of Frequency is -.03.  Again, this is the effect of Frequency in the English condition-- compare to the other model!

In a model where a level of the variable corresponds to zero, the main effects and interactions are the amount bigger or smaller that each condition is compared to the English effects.

Note that this model gives different information:  We no longer have a German vs Spanish comparison.  This is why setting the right contrasts is important: use your model to test the question you care about.


## Simple effect contrasts
A contrast coding scheme that tests effects with respect to the average level is called simple effects coding. This is also straighforward to implement. It's like the dummy coding matrix, but instead of zeroes, we use -1/k (here, -1/4). Instead of ones, we use (k-1)/k.

```{r simple, include=T}
#### Set simple effects coding contrasts ####
### reference level = still English
## make new contrast matrix with named values
Dutch <- c(3/4,-1/4,-1/4,-1/4) 
Ger <- c(-1/4,-1/4,3/4,-1/4)
Spa <- c(-1/4,-1/4,-1/4,3/4)

Seff <- cbind(Dutch,Ger,Spa)
rownames(Seff)=c("Dutch","English","German","Spanish")

## set these contrasts
contrasts(lexdecSm$NativeLangSim) <- Seff

### run analysis
lmSimple <- lmer(RT~NativeLangSim*FreqC + (1+FreqC|Subject)+(1|Word),data=lexdecSm)
summary(lmSimple)

```

The model is overall somewhat similar to the dummy-coded one, but all effects relate to the average.  The intercept is the average response time across all conditions, and the effect of Dutch, German, and Spanish reflects the difference between these and the average value.

Note that the effect of Frequency now matches the first model we ran, with sum-coding.  This is because all efffects are evaluated at the average response time-- including Frequency.Here, this is the average effect of Frequency-- not the effect of Frequency for English speakers (as it was in the most-recent model). 

This is key: contrasts affect the other variables too, since they affect what the intercept means.  Make sure to set the right contrasts for comparing ALL variables at the right level.

## Helmert
A variant on sum coding is Helmert coding. This is useful for ordered factors. Here, the first comparison tests one level versus everything 'larger' than it (ordinally), by comparing (k-1) to -1. The second comparsion replaces the (k-1) with a zero, and then compares the next largest level to everything 'larger', by setting those to -1. And so forth, until you're only comparing two things!

```{r helmert, include=T}
## make new contrast matrix with named values
## remember, the levels are out of order!
H1 <- c(-1,3,-1,-1) 
H2 <- c(2,0,-1,-1)
H3 <- c(0,0,1,-1)

Helm <- cbind(H1,H2,H3)
rownames(Helm)=c("Dutch","English","German","Spanish")

#set contrasts
contrasts(lexdecSm$NativeLangSim) <- Helm

## if you have things ordered in the right way, this is equivalent to
#contrasts(lexdecSm$NativeLangSim) <- contr.helmert 

### run analysis
lmHelm <- lmer(RT~NativeLangSim*FreqC + (1+FreqC|Subject)+(1 |Word),data=lexdecSm)
summary(lmHelm)

### replot main effect of native language only, emphasizing order by re-ordering
ggplot(lexdecSm,aes(x=reorder(NativeLangSim,RT,fun=median),y=RT,fill=NativeLangSim))+
  geom_boxplot() + scale_x_discrete("Native Language")
```

The intercept is the same as in the sum coding model and the simple effects model; same for the main effect of frequency. The main effects at each contrast are different. Contrast 1 reflects the difference between the English RT and everything else; contrast 2 reflects Dutch vs German and Spanish; and contrast 3 reflects German vs Spanish. 

But! They don't reflect the numeric difference. They reflect the average difference in the levels compared. This says that the average  difference between English and all others is roughly the same as Dutch and (Spanish, German), but that the Spanish-German difference is smaller.

What's nice about this model is especailly the way we can interpret the interactions. Note that the estimate for contrast 1 by frequency is the largest-- that reflects how English has a flatter slope for Frequency than all the other groups.  (We saw that in the panel plot above!)

## More Helmert 

To get the average condition-wise differences, you'd run a different model-- one where the absolute value of all weights on the contrast sums to 1.  That's what's implemented below.  Note that the contrast betas differ, but the t-values are identical. That's just because we're changing the weights of what we want to average over- not the amount of variability in the model.


```{r helmert2, include=T}
## make new contrast matrix with named values
## remember, the levels are out of order!
H1 <- c(-1/6,1/2,-1/6,-1/6) 
H2 <- c(1/2,0,-1/4,-1/4)
H3 <- c(0,0,1/2,-1/2)

Helm2 <- cbind(H1,H2,H3)
rownames(Helm)=c("Dutch","English","German","Spanish")

#set contrasts
contrasts(lexdecSm$NativeLangSim) <- Helm2

### run analysis
lmHelm2 <- lmer(RT~NativeLangSim*FreqC + (1+FreqC|Subject)+(1 |Word),data=lexdecSm)
summary(lmHelm2)
```


## In-class Activity: Titanic

Returning to the Titanic data set, we can see that contrasts matter a lot for interpretation.  Here is a cross-tab of the data (proportion survived for each of women and men by passenger class 1-3). 

Here are two models, and their contrasts. Note their differences, and what inferences we can draw from each.  Note especially: the intercept! Walk through this code with your neighbor to start-- then we'll discuss in class together.

In this code, we will use built-in contrasts in R since they'll work for this model.  It's a nice shortcut when you can take it!

```{r TitanicContrasts1, include=T}
TitanicAll <- read.csv('TitanicAll.csv',header=T, sep="\t")

TitanicAll$Pclass <- as.factor(TitanicAll$Pclass)

TitanicAll %>% group_by(Sex,Pclass) %>% summarise(mean(Survived))

```

```{r TitanicContrasts2, include=T}
contrasts(TitanicAll$Sex) <- contr.treatment ## Use treatment coding, aka dummy coding
contrasts(TitanicAll$Pclass) <- contr.treatment

contrasts(TitanicAll$Sex) 
contrasts(TitanicAll$Pclass) 
T1 <- glm(Survived ~ Sex*Pclass, family='binomial',data=TitanicAll)
summary(T1)
```

```{r TitanicContrasts3, include=T}

contrasts(TitanicAll$Sex) <- contr.helmert ## use helmert
contrasts(TitanicAll$Pclass) <- contr.helmert

contrasts(TitanicAll$Sex) 
contrasts(TitanicAll$Pclass) 
T2 <- glm(Survived ~ Sex*Pclass, family='binomial',data=TitanicAll)

summary(T2)


